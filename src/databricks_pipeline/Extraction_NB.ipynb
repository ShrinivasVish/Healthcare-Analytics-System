{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%run ./Global_Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%run ./Layer_Utilities_NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class DataExtractor:\n",
    "    \"\"\"\n",
    "    Extracts data from an AWS DynamoDB table and loads it into a Bronze layer in a data lake.\n",
    "    Supports both full and incremental extraction based on the `modified_at` timestamp.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dynamo_table_name, bronze_layer_path, aws_access_key, aws_secret_access_key, region_name, max_retries=3, retry_delay=5):\n",
    "        \"\"\"\n",
    "        Initializes the DataExtractor class with AWS credentials, Spark session, and table details.\n",
    "\n",
    "        Args:\n",
    "            dynamo_table_name (str): Name of the DynamoDB table to extract data from.\n",
    "            bronze_layer_path (str): Path to the Bronze layer where extracted data will be stored.\n",
    "            aws_access_key (str): AWS access key for authentication.\n",
    "            aws_secret_access_key (str): AWS secret access key for authentication.\n",
    "            region_name (str): AWS region where the DynamoDB table is hosted.\n",
    "            max_retries (int): Maximum number of retry attempts for connections.\n",
    "            retry_delay (int): Delay (in seconds) between retries.\n",
    "        \"\"\"\n",
    "        self.dynamo_table_name = dynamo_table_name\n",
    "        self.bronze_layer_path = bronze_layer_path\n",
    "        self.max_retries = max_retries\n",
    "        self.retry_delay = retry_delay\n",
    "\n",
    "        # Initialize Spark session with retry logic\n",
    "        self.spark = self._initialize_spark()\n",
    "\n",
    "        # Initialize DynamoDB client with retry logic\n",
    "        self.dynamodb_client = self._initialize_dynamodb(aws_access_key, aws_secret_access_key, region_name)\n",
    "\n",
    "    def _initialize_spark(self):\n",
    "        \"\"\"Retries Spark session initialization in case of failures.\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                return SparkSession.builder.appName(\"DataExtractor\").getOrCreate()\n",
    "            except Exception as e:\n",
    "                print(f\"Spark initialization failed (Attempt {attempt + 1}/{self.max_retries}): {str(e)}\")\n",
    "                time.sleep(self.retry_delay)\n",
    "        raise Exception(\"Failed to initialize Spark after multiple attempts.\")\n",
    "\n",
    "    def _initialize_dynamodb(self, aws_access_key, aws_secret_access_key, region_name):\n",
    "        \"\"\"Retries DynamoDB client initialization in case of failures.\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                return boto3.client(\n",
    "                    \"dynamodb\",\n",
    "                    aws_access_key_id=aws_access_key,\n",
    "                    aws_secret_access_key=aws_secret_access_key,\n",
    "                    region_name=region_name,\n",
    "                )\n",
    "            except (BotoCoreError, NoCredentialsError, ClientError) as e:\n",
    "                print(f\"DynamoDB initialization failed (Attempt {attempt + 1}/{self.max_retries}): {str(e)}\")\n",
    "                time.sleep(self.retry_delay)\n",
    "        raise Exception(\"Failed to initialize DynamoDB after multiple attempts.\")\n",
    "\n",
    "    def _get_dynamo_data(self, filter_expression=None):\n",
    "        \"\"\"Extracts data from DynamoDB with optional filtering.\"\"\"\n",
    "        scan_kwargs = {}\n",
    "        if filter_expression:\n",
    "            scan_kwargs[\"FilterExpression\"] = filter_expression\n",
    "\n",
    "        items = []\n",
    "        while True:\n",
    "            try:\n",
    "                response = self.dynamodb_client.scan(TableName=self.dynamo_table_name, **scan_kwargs)\n",
    "                items.extend(response.get(\"Items\", []))\n",
    "                \n",
    "                if \"LastEvaluatedKey\" not in response:\n",
    "                    break\n",
    "\n",
    "                scan_kwargs[\"ExclusiveStartKey\"] = response[\"LastEvaluatedKey\"]\n",
    "\n",
    "            except (BotoCoreError, ClientError) as e:\n",
    "                print(f\"Error scanning DynamoDB: {str(e)}\")\n",
    "                break  # Exit loop on failure\n",
    "\n",
    "        if not items:\n",
    "            return self.spark.createDataFrame([], self.spark.createDataFrame([{}]).schema)\n",
    "\n",
    "        # Convert DynamoDB items to Spark DataFrame\n",
    "        data = [{k: list(v.values())[0] for k, v in item.items()} for item in items]\n",
    "        return self.spark.createDataFrame(data)\n",
    "    \n",
    "    def extract_data(self):\n",
    "        \"\"\"\n",
    "        Extracts data from the DynamoDB table and loads it into the Bronze layer.\n",
    "        \n",
    "        - If the Bronze layer is empty, performs a full extraction.\n",
    "        - If the Bronze layer contains data, performs an incremental extraction based on `modified_at`.\n",
    "        \n",
    "        Logs extraction status in the `gold.etl_tracker` table only after extraction completion.\n",
    "        \"\"\"\n",
    "        source_layer = \"source\"\n",
    "        destination_layer = \"bronze\"\n",
    "        source_table = self.dynamo_table_name\n",
    "        destination_table = self.bronze_layer_path\n",
    "        current_timestamp = datetime.utcnow().isoformat()\n",
    "\n",
    "        try:\n",
    "            if LayerUtils.is_layer_empty(self.bronze_layer_path):\n",
    "                print(\"Bronze layer is empty. Performing full extraction...\")\n",
    "                data_df = self._get_dynamo_data()\n",
    "            else:\n",
    "                print(\"Bronze layer is not empty. Performing incremental extraction...\")\n",
    "                bronze_data = self._safe_read_parquet(self.bronze_layer_path)\n",
    "\n",
    "                # Get the latest `modified_at` timestamp from Bronze layer\n",
    "                latest_modified_at = bronze_data.agg({\"modified_at\": \"max\"}).collect()[0][0]\n",
    "\n",
    "                # Define filter for incremental extraction\n",
    "                filter_expression = f\"modified_at > '{latest_modified_at}' AND modified_at <= '{current_timestamp}'\"\n",
    "                data_df = self._get_dynamo_data(filter_expression)\n",
    "\n",
    "            # Write extracted data to Bronze layer\n",
    "            extracted_count = data_df.count()\n",
    "            self._write_to_bronze_layer(data_df)\n",
    "\n",
    "            # Log success **AFTER** extraction and writing are complete\n",
    "            LayerUtils.log_etl_status(self.spark, source_layer, destination_layer, source_table, destination_table, \"SUCCESS\", extracted_count)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during extraction: {str(e)}\")\n",
    "\n",
    "            # Log failure in case of an error\n",
    "            LayerUtils.log_etl_status(self.spark, source_layer, destination_layer, source_table, destination_table, \"FAILED\", 0)\n",
    "\n",
    "    def _safe_read_parquet(self, path):\n",
    "        \"\"\"Reads a Parquet file safely, handling missing/corrupt files gracefully.\"\"\"\n",
    "        try:\n",
    "            return self.spark.read.parquet(path)\n",
    "        except AnalysisException as e:\n",
    "            print(f\"Error reading Parquet file: {str(e)}\")\n",
    "            return self.spark.createDataFrame([], self.spark.read.parquet(path).schema)\n",
    "\n",
    "    def _write_to_bronze_layer(self, data_df):\n",
    "        \"\"\"\n",
    "        Writes extracted data to the Bronze layer.\n",
    "        \n",
    "        - Overwrites for full extraction.\n",
    "        - Appends for incremental extraction.\n",
    "        \"\"\"\n",
    "        LayerUtils.write_to_layer(data_df, self.bronze_layer_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
