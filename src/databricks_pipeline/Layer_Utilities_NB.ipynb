{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%run ./Global_Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class LayerUtils:\n",
    "\n",
    "    folder_name_for_dbfs = \"\"\n",
    "    file_name_for_s3 = \"\"\n",
    "\n",
    "    redshift_url = \"jdbc:redshift://your-cluster-name.region-name.redshift.amazonaws.com:port/database-name\"\n",
    "    etl_trakcer_tbl = 'gold.etl_tracker'\n",
    "    redshift_user = \"redshift-user\"\n",
    "    redshift_password = \"redshift-password\"\n",
    "    redshift_driver = 'com.amazon.redshift.jdbc.Driver'\n",
    "\n",
    "    etl_log_df.write.jdbc(\n",
    "        url=LayerUtils.redshift_url,\n",
    "        table=LayerUtils.etl_trakcer_tbl,\n",
    "        mode=\"append\",\n",
    "        properties={\n",
    "            \"user\": LayerUtils.redshift_user,\n",
    "            \"password\": LayerUtils.redshift_password,\n",
    "            \"driver\": LayerUtils.redshift_driver\n",
    "        }\n",
    "    )\n",
    "\n",
    "    max_retries = 3  # Define as class variable\n",
    "    retry_delay = 5  # Define retry delay in seconds\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_spark():\n",
    "        \"\"\"Retries Spark session initialization in case of failures.\"\"\"\n",
    "        for attempt in range(LayerUtils.max_retries):\n",
    "            try:\n",
    "                return SparkSession.builder.appName(\"DataExtractor\").getOrCreate()\n",
    "            except Exception as e:\n",
    "                print(f\"Spark initialization failed (Attempt {attempt + 1}/{LayerUtils.max_retries}): {str(e)}\")\n",
    "                time.sleep(LayerUtils.retry_delay)  # Use class variable\n",
    "        raise Exception(\"Failed to initialize Spark after multiple attempts.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def is_layer_empty(layer_path: str) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if the specified layer directory in DBFS is empty.\n",
    "\n",
    "        Args:\n",
    "            layer_path (str): The path to the layer directory.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the layer directory is empty or doesn't exist; otherwise, False.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            files = dbutils.fs.ls(layer_path)\n",
    "            return len(files) == 0  # True if no files exist\n",
    "        except Exception:\n",
    "            return True\n",
    "\n",
    "    @staticmethod\n",
    "    def write_to_layer(data_df: DataFrame, layer_path: str):\n",
    "        \"\"\"\n",
    "        Writes the processed data to the specified layer directory.\n",
    "\n",
    "        - For full extraction: Overwrites existing data in the directory.\n",
    "        - For incremental extraction: Writes new records to a new file.\n",
    "        \"\"\"\n",
    "        if LayerUtils.is_layer_empty(layer_path):\n",
    "            # LayerUtils.folder_name_for_dbfs = \"full_extraction.parquet\"\n",
    "            LayerUtils.folder_name_for_dbfs = \"full_extraction_mini.parquet\"\n",
    "        else:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            LayerUtils.folder_name_for_dbfs = f\"incremental_data_{timestamp}.parquet\"\n",
    "        \n",
    "        # Coalesce the DataFrame to a single partition\n",
    "        single_partition_df = data_df.coalesce(1)\n",
    "\n",
    "        # Define temporary output path\n",
    "        temp_output_path = f\"{layer_path}_temp\"\n",
    "\n",
    "        # Write the DataFrame to the temporary path\n",
    "        single_partition_df.write.mode(\"overwrite\").parquet(temp_output_path)\n",
    "\n",
    "        # Move the file from the temporary path to the final path with the correct file name\n",
    "        files = dbutils.fs.ls(temp_output_path)\n",
    "        # for file in files:\n",
    "        #     if file.name.endswith('.parquet'):\n",
    "        #         dbutils.fs.mv(file.path, f\"{layer_path}/{LayerUtils.folder_name_for_dbfs}\")\n",
    "        #         break\n",
    "        for file in files:\n",
    "            if file.name.endswith('.parquet'):\n",
    "                destination_path = f\"{layer_path}/{LayerUtils.folder_name_for_dbfs}\"\n",
    "                # Copy the file to the destination\n",
    "                dbutils.fs.cp(file.path, destination_path)\n",
    "                # Delete the original file from the temporary location\n",
    "                dbutils.fs.rm(file.path, recurse=True)\n",
    "                break\n",
    "\n",
    "        # Clean up the temporary directory\n",
    "        dbutils.fs.rm(temp_output_path, True)\n",
    "\n",
    "        print(f\"Data written to layer as {LayerUtils.folder_name_for_dbfs}.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def is_s3_bucket_empty(bucket_name: str, directory: str = \"\") -> bool:\n",
    "        \"\"\"\n",
    "        Checks if the specified directory inside an S3 bucket is empty.\n",
    "        If directory is empty, it checks the entire bucket.\n",
    "        \"\"\"\n",
    "        s3 = boto3.client('s3')\n",
    "        response = s3.list_objects_v2(Bucket=bucket_name, Prefix=directory)\n",
    "\n",
    "        # If 'Contents' is not in the response, the bucket/directory is empty\n",
    "        if 'Contents' not in response:\n",
    "            return True\n",
    "\n",
    "        # Check if any actual files exist in the directory\n",
    "        return all(obj['Key'].endswith(\"/\") for obj in response['Contents'])  # True if only folder placeholders exist\n",
    "\n",
    "    @staticmethod\n",
    "    def write_to_s3(data_df: DataFrame, bucket_name: str, s3_path: str):\n",
    "        \"\"\"\n",
    "        Transfers data from the Bronze layer to the Silver layer (S3).\n",
    "\n",
    "        - Writes the data first to DBFS\n",
    "        - Renames and moves it to the correct S3 location\n",
    "        \"\"\"\n",
    "        \n",
    "        source_layer = \"bronze\"\n",
    "        destination_layer = \"silver\"\n",
    "        source_table = \".parquet file\"  # Update with actual Bronze table name\n",
    "        destination_table = \".parquet file\"  # Update with actual Silver table name\n",
    "\n",
    "        spark = LayerUtils.initialize_spark()\n",
    "        \n",
    "        # Log ETL Start\n",
    "        LayerUtils.log_etl_status(source_layer, destination_layer, source_table, destination_table, \"IN_PROGRESS\", 0)\n",
    "\n",
    "        try:\n",
    "            # Check if Silver Layer is Empty\n",
    "            if LayerUtils.is_s3_bucket_empty(bucket_name, bucket_path):\n",
    "                print(\"Silver layer is empty. Performing full transfer...\")\n",
    "                # bronze_data = spark.read.parquet(bronze_layer_path)\n",
    "                output_filename = \"full_extraction_mini.parquet\"\n",
    "            else:\n",
    "                print(\"Silver layer is not empty. Performing incremental transfer...\")\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                output_filename = f\"incremental_data_{timestamp}.parquet\"\n",
    "\n",
    "                # Get max `modified_at` from Silver layer\n",
    "                latest_modified_at_from_silver = get_latest_modified_at_from_s3(bucket_name, bucket_path)\n",
    "\n",
    "                # Filter Bronze data for incremental transfer\n",
    "                data_df = data_df.filter(data_df[\"modified_at\"] > latest_modified_at_from_silver)\n",
    "\n",
    "                # Generate timestamped filename for incremental load\n",
    "                output_filename = f\"{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet\"\n",
    "            \n",
    "            # Count records before transfer\n",
    "            record_count = data_df.count()\n",
    "            \n",
    "            # Step 1: Write to Temporary Location in DBFS\n",
    "            temp_dbfs_path = \"dbfs:/tmp/silver_layer_temp\"\n",
    "            data_df.coalesce(1).write.mode(\"overwrite\").parquet(temp_dbfs_path)\n",
    "            print(f\"Data written temporarily to {temp_dbfs_path}\")\n",
    "\n",
    "            # Step 2: Fetch the generated Parquet file from DBFS\n",
    "            temp_files = dbutils.fs.ls(temp_dbfs_path)\n",
    "            parquet_file = None\n",
    "\n",
    "            for file in temp_files:\n",
    "                if file.name.endswith(\".parquet\"):\n",
    "                    parquet_file = file.path\n",
    "                    break\n",
    "            \n",
    "            if not parquet_file:\n",
    "                raise Exception(\"No Parquet file found in the temporary DBFS location!\")\n",
    "\n",
    "            # Step 3: Move the File to S3\n",
    "            final_s3_path = f\"s3://{bucket_name}/{bucket_path}/{output_filename}\"\n",
    "\n",
    "            print()\n",
    "            print(f\"bucket_name: {bucket_name}\")\n",
    "            print(f\"bucket_path: {bucket_path}\")\n",
    "            print(f\"output_filename: {output_filename}\")\n",
    "            print()\n",
    "            \n",
    "            dbutils.fs.cp(parquet_file, final_s3_path)\n",
    "            print(f\"File moved to final destination: {final_s3_path}\")\n",
    "\n",
    "            # Log Success in ETL Tracker\n",
    "            LayerUtils.log_etl_status(source_layer, destination_layer, source_table, destination_table, \"SUCCESS\", record_count)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during transfer: {str(e)}\")\n",
    "\n",
    "            # Log Failure in ETL Tracker\n",
    "            LayerUtils.log_etl_status(source_layer, destination_layer, source_table, destination_table, \"FAILED\", 0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_from_bronze_layer():\n",
    "        \"\"\"\n",
    "        Reads all Parquet files from the specified layer path and returns a Spark DataFrame.\n",
    "\n",
    "        Args:\n",
    "            layer_path (str): The path to the layer directory.\n",
    "        \n",
    "        Returns:\n",
    "            pyspark.sql.DataFrame: A Spark DataFrame containing the data from the Parquet files.\n",
    "        \"\"\"\n",
    "        spark = LayerUtils.initialize_spark()\n",
    "        return spark.read.parquet(f\"{bronze_layer_path}/{LayerUtils.folder_name_for_dbfs}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_source_count_from_dynamodb(source_table):\n",
    "        \"\"\"\n",
    "        Fetches the record count of the source table from DynamoDB using scan.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dynamo_db_client = boto3.client(\n",
    "                \"dynamodb\",\n",
    "                aws_access_key_id=aws_access_key,\n",
    "                aws_secret_access_key=aws_secret_access_key,\n",
    "                region_name=region_name,\n",
    "            )\n",
    "\n",
    "            # Use scan to count all records\n",
    "            response = dynamo_db_client.scan(\n",
    "                TableName=\"tbl_healthcare_analytics_data_mini\",\n",
    "                Select=\"COUNT\"  # Optimized for counting items\n",
    "            )\n",
    "\n",
    "            return response.get('Count', 0)  # Return count of records\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching count from DynamoDB: {e}\")\n",
    "            return -1  # Indicate error with -1\n",
    "\n",
    "    @staticmethod\n",
    "    def get_count_from_dbfs(self, bronze_table_path = bronze_layer_path):\n",
    "        \"\"\"\n",
    "        Fetches the record count from a Delta table stored in DBFS (Bronze Layer).\n",
    "        Uses is_layer_empty() to check if the layer directory exists or is empty.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Initialize a Spark session\n",
    "            spark = SparkSession.builder.appName(\"DataExtractor\").getOrCreate()\n",
    "\n",
    "            # Check if the directory is empty\n",
    "            if LayerUtils.is_layer_empty(bronze_table_path):\n",
    "                print(f\"Bronze layer is empty or does not exist: {bronze_table_path}\")\n",
    "                return 0\n",
    "\n",
    "            # Read all Parquet files from the directory\n",
    "            df = spark.read.format(\"parquet\").load(bronze_table_path)\n",
    "\n",
    "            if df.isEmpty():\n",
    "                print(f\"No data found in the Bronze layer path: {bronze_table_path}\")\n",
    "                return 0\n",
    "\n",
    "            # Add file path column\n",
    "            df = df.withColumn(\"file_path\", input_file_name())\n",
    "\n",
    "            # Extract the latest file based on filename timestamp\n",
    "            latest_file = (\n",
    "                df.select(\"file_path\")\n",
    "                .distinct()\n",
    "                .orderBy(desc(\"file_path\"))\n",
    "                .limit(1)\n",
    "                .collect()\n",
    "            )\n",
    "\n",
    "            if not latest_file:\n",
    "                print(\"No valid files found in the specified Bronze layer path.\")\n",
    "                return 0\n",
    "\n",
    "            latest_file_path = latest_file[0][\"file_path\"]\n",
    "            print(f\"Latest Parquet file detected: {latest_file_path}\")\n",
    "\n",
    "            # Read only the latest file\n",
    "            latest_df = spark.read.format(\"parquet\").load(latest_file_path)\n",
    "\n",
    "            return latest_df.count()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching count from DBFS: {e}\")\n",
    "            return -1\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_count_from_s3(self, staging_table_path = f\"s3://{bucket_name}/{bucket_path}\"):\n",
    "        \"\"\"\n",
    "        Fetches the record count from a Delta table stored in S3 (Staging Layer).\n",
    "        Uses is_s3_bucket_empty() to check if the bucket/directory is empty before proceeding.\n",
    "        \"\"\"\n",
    "\n",
    "        spark = LayerUtils.initialize_spark()\n",
    "\n",
    "        try:\n",
    "            # Extract bucket name and prefix (directory) from the S3 path\n",
    "            bucket_name = staging_table_path.split(\"//\")[1].split(\"/\")[0]\n",
    "            directory = \"/\".join(staging_table_path.split(\"//\")[1].split(\"/\")[1:])\n",
    "\n",
    "            # Use the existing method to check if the S3 bucket/directory is empty\n",
    "            if LayerUtils.is_s3_bucket_empty(bucket_name, directory):\n",
    "                print(f\"S3 bucket is empty or does not contain valid data: {staging_table_path}\")\n",
    "                return 0  # Return 0 if the bucket or directory is empty\n",
    "\n",
    "            # Read the Delta table from S3\n",
    "            df = spark.read.format(\"delta\").load(staging_table_path)\n",
    "\n",
    "            # Check if a timestamp column exists\n",
    "            if \"modified_at\" in df.columns:\n",
    "                latest_timestamp = df.select(spark_max(col(\"modified_at\"))).collect()[0][0]\n",
    "\n",
    "                if latest_timestamp is not None:\n",
    "                    df = df.filter(col(\"modified_at\") == latest_timestamp)\n",
    "\n",
    "            return df.count()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching count from S3: {e}\")\n",
    "            return None  # Indicate error\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_latest_modified_at_from_s3(bucket_name, bucket_path):\n",
    "        \"\"\"\n",
    "        Retrieves the latest modified timestamp from an S3 bucket path.\n",
    "\n",
    "        :param bucket_name: Name of the S3 bucket.\n",
    "        :param bucket_path: Prefix or folder path in the S3 bucket.\n",
    "        :return: Latest modified timestamp as a string in 'YYYY-MM-DD HH:MM:SS' format or None if no files exist.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            s3_client = boto3.client('s3')\n",
    "            response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=bucket_path)\n",
    "            \n",
    "            if 'Contents' not in response:\n",
    "                print(f\"No files found in S3 path: s3://{bucket_name}/{bucket_path}\")\n",
    "                return None\n",
    "            \n",
    "            latest_file = max(response['Contents'], key=lambda x: x['LastModified'])\n",
    "            latest_modified_at = latest_file['LastModified']\n",
    "            \n",
    "            # Convert to string format\n",
    "            return latest_modified_at.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving latest modified timestamp from S3: {e}\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def log_etl_status(spark, source_layer, destination_layer, source_table, destination_table, status, source_path=None, dest_path=None):\n",
    "        \"\"\"\n",
    "        Logs ETL progress and performs reconciliation.\n",
    "\n",
    "        Args:\n",
    "            source_layer (str): Source layer name (e.g., \"source\", \"bronze\").\n",
    "            destination_layer (str): Destination layer name (e.g., \"bronze\", \"staging\").\n",
    "            source_table (str): Source table name.\n",
    "            destination_table (str): Destination table name.\n",
    "            status (str): Status of the ETL process (e.g., \"SUCCESS\", \"FAILED\").\n",
    "            source_path (str, optional): Path to the source dataset (DBFS for bronze, S3 for staging).\n",
    "            dest_path (str, optional): Path to the destination dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get current UTC timestamp\n",
    "        utc_now = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        spark = LayerUtils.initialize_spark()\n",
    "\n",
    "        if source_layer == \"source\" and destination_layer == \"bronze\":\n",
    "            source_count = LayerUtils.get_source_count_from_dynamodb(source_table)  # Fetch from DynamoDB\n",
    "            destination_count = LayerUtils.get_count_from_dbfs(source_path)  # Fetch from DBFS\n",
    "        elif source_layer == \"bronze\" and destination_layer == \"staging\":\n",
    "            source_count = LayerUtils.get_count_from_dbfs(source_path)  # Fetch from DBFS\n",
    "            destination_count = LayerUtils.get_count_from_s3(dest_path)  # Fetch from S3\n",
    "        else:\n",
    "            source_count = None\n",
    "            destination_count = None\n",
    "\n",
    "        # Handle None values explicitly\n",
    "        source_count = source_count if source_count is not None else -1\n",
    "        destination_count = destination_count if destination_count is not None else -1\n",
    "\n",
    "        # Determine reconciliation status\n",
    "        if source_count == -1 or destination_count == -1:\n",
    "            reconciliation_status = \"SKIPPED\"\n",
    "        elif source_count == destination_count:\n",
    "            reconciliation_status = \"MATCH\"\n",
    "        else:\n",
    "            reconciliation_status = \"MISMATCH\"\n",
    "\n",
    "        # Define schema explicitly\n",
    "        schema = StructType([\n",
    "            StructField(\"source_layer\", StringType(), False),\n",
    "            StructField(\"destination_layer\", StringType(), False),\n",
    "            StructField(\"source_table\", StringType(), False),\n",
    "            StructField(\"destination_table\", StringType(), False),\n",
    "            StructField(\"status\", StringType(), False),\n",
    "            StructField(\"source_count\", IntegerType(), True),\n",
    "            StructField(\"destination_count\", IntegerType(), True),\n",
    "            StructField(\"reconciliation_status\", StringType(), False),  # Corrected column name\n",
    "            StructField(\"completion_timestamp\", StringType(), False)\n",
    "        ])\n",
    "\n",
    "        # Create DataFrame with explicit schema\n",
    "        etl_log_df = spark.createDataFrame(\n",
    "            [(source_layer, destination_layer, source_table, destination_table, status, source_count, destination_count, reconciliation_status, utc_now)],\n",
    "            schema\n",
    "        )\n",
    "\n",
    "        \n",
    "        etl_log_df.write.jdbc(\n",
    "            url=LayerUtils.redshift_url,\n",
    "            table=LayerUtils.etl_trakcer_tbl,\n",
    "            mode=\"append\",\n",
    "            properties={\n",
    "                \"user\": LayerUtils.redshift_user,\n",
    "                \"password\": LayerUtils.redshift_password,\n",
    "                \"driver\": LayerUtils.redshift_driver\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(f\"ETL status logged: {source_layer} → {destination_layer}, {source_table} → {destination_table}, Status: {status}, Recon: {reconciliation_status}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
