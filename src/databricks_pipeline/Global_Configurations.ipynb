{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.dataframe import *\n",
    "from pyspark.sql.window import *\n",
    "\n",
    "from botocore.exceptions import BotoCoreError, NoCredentialsError, ClientError\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "import time\n",
    "import json\n",
    "import pytz\n",
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Layer Paths\n",
    "bronze_layer_path = \"dbfs:/FileStore/tables/healthcare_analytics_system/bronze_layer\"\n",
    "silver_layer_path = \"dbfs:/FileStore/tables/healthcare_analytics_system/silver_layer\"\n",
    "gold_layer_path = \"dbfs:/FileStore/tables/healthcare_analytics_system/gold_layer\"\n",
    "\n",
    "# Define the parameters for extraction\n",
    "dynamo_table_name = \"tbl_healthcare_analytics_data_mini\"\n",
    "region_name = \"<aws-region>\"\n",
    "\n",
    "bucket_name = \"healthcare-analytics-data\"\n",
    "bucket_path = \"silver-layer\"\n",
    "file_name_in_bucket = \"healthcare-analytics-data-silver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "os.environ['AWS_ACCESS_KEY_ID'] = \"<aws-access_key>\"\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = \"<aws-secret-access-key>\"\n",
    "\n",
    "# Configure Spark to use the credentials\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", os.environ['AWS_ACCESS_KEY_ID'])\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", os.environ['AWS_SECRET_ACCESS_KEY'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
