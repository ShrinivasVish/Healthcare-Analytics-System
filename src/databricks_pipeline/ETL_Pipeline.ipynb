{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%run ./Global_Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%run ./Layer_Utilities_NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%run ./Extraction_NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%run ./Transformation_NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ETLPipeline:\n",
    "    def __init__(self, extractor, transformer):\n",
    "        \"\"\"\n",
    "        Initializes the ETL pipeline with necessary components.\n",
    "\n",
    "        Args:\n",
    "            extractor (DataExtractor): The data extraction class instance.\n",
    "            s3_target_path (str): The S3 path where the final processed data will be stored.\n",
    "        \"\"\"\n",
    "        self.extractor = extractor\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        \"\"\"\n",
    "        Runs the ETL pipeline in the following sequence:\n",
    "        1. Extracts data from DynamoDB and writes it to the Bronze layer.\n",
    "        2. Reads data from the Bronze layer.\n",
    "        3. Applies transformations.\n",
    "        4. Writes transformed data to the S3 bucket.\n",
    "        \"\"\"\n",
    "        # LayerUtils.create_etl_tracker_table()\n",
    "\n",
    "        print(\"\\nStarting ETL pipeline...\")\n",
    "\n",
    "        # Step 1: Extract data from DynamoDB and write to the Bronze layer\n",
    "        print(\"\\nExtracting data from DynamoDB...\")\n",
    "        self.extractor.extract_data()\n",
    "\n",
    "        # Step 2: Read data from the Bronze layer\n",
    "        print(\"\\nReading data from the Bronze layer...\")\n",
    "        bronze_df = LayerUtils.read_from_bronze_layer()\n",
    "\n",
    "        # Step 3: Apply transformations\n",
    "        print(\"\\nApplying transformations...\")\n",
    "        transformed_df = self.transformer.process_dataframe(bronze_df)\n",
    "\n",
    "        print(transformed_df.columns)\n",
    "\n",
    "        # Step 4: Write transformed data to S3\n",
    "        print(\"\\nWriting transformed data to S3...\")\n",
    "        LayerUtils.write_to_s3(transformed_df, bucket_name, bucket_path)\n",
    "\n",
    "        print(\"\\nETL pipeline completed successfully!\")\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data_extractor = DataExtractor(dynamo_table_name, bronze_layer_path, aws_access_key, aws_secret_access_key, region_name)\n",
    "data_transformer = DataTransformer()\n",
    "etl_pipeline = ETLPipeline(data_extractor, data_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "etl_pipeline.run_pipeline()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
